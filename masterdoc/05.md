### Exercise 3: Explore Machine Learning and Business Intelligence scenarios on the Lakehouse. <a name="data-science-and-analytics-on-the-Lakehouse"></a>

#### Task 3.1: Review MLOps pipeline using the Azure Databricks managed MLflow and Operationalized as an ML service in Azure ML<a name="ml-model-using-ml-flow"></a>

Now that we've ingested and processed our customer data, we want to understand what makes one customer more likely to churn than another, and ultimately see if we can produce a machine learning model that can accurately predict if a particular customer will churn.

We would also like to understand our customer's sentiment, so that we can create targeted campaigns to improve our sales.

*Architecture Diagram*

![Architecture Diagram](media/images/image3100.png)

1. In the Databricks web session (tab), which you opened in Exercise 2, in the left pane, in the top dropdown list, select the **Machine Learning** persona.

*By default, the left pane appears in a collapsed state and only the icons are visible. You can hover your cursor over the pane to expand it for the full view.*

![Select the persona](media/images/image3101.png)

2. In the left pane, select the **Workspace** icon (first icon), and then select the **ML Solutions in OneBox** notebook. Once notebook is open, click on > to expand notebooks Table of content.

![Select the workspaces](media/images/image3102.png)

In this task, you wonâ€™t run any cells. Instead, you will explore the selected cells and review their outcomes.
In the scatter-plot chart below, you can see the corelation between:

- Customer churn,
- Amount spent by the customer and
- Number of months as a customer (tenure).

  Here you will find that most of the customers who churn are those, who spend the least amount of money and stay for fewer months.

3. Review the **cmd 11** cell, and then review the output for **Exploratory Data Analysis**.

   ![cmd 11](media/images/image3103.png)

4. Review the **cmd 29** cell.

By analyzing Customer Churn data, we create multiple Machine Learning models.

![Runs using a parallel coordinates](media/images/image3115.png)

The best ML model for Customer Churn is selected and registered with Azure model registry.

**Operationalized as an ML service using MLOps in Azure ML/AI**.

5. Review the **cmd 44** cell to load Azure ML Workspace.

![Load Azure ML workspace](media/images/image3119.png)

The MLflow plugin azureml-mlflow is used to deploy the Customer churn ML model to Azure ML. The deployed ML model will be used to predict Customer churn.

**Twitter Sentiment Score Custom ML Model**

*This model helps Wide World Importers to analyze the sentiment of their customers based on what's trending on social media platforms like Twitter. The popular social media sentiments can then be used to create effective marketing campaigns for the target audience.*

6. In the left, select the small arrow to expand the sidebar, collapse the **Customer Churn Model** and select **Twitter Sentiment Score Model**.

   ![Twitter sentiment score model](media/images/image3123.png)

7. Review the **cmd 75** cell for training and validation of customer sentiment model.
*Here the Twitter sentiment model is trained for further consumption.*

   ![Customer model train and validation](media/images/image3126.png)


**Campaign Analytics**

8. From the sidebar, select **Campaign Analytics**.

   ![Campaign Analytics](media/images/image3128.png)

9. Review the **cmd 88** cell.

*Wide World Importers decided to run a number of campaigns using the Twitter sentiment model which was trained in cmd 75 cell in order to reduce customer churn and increase revenue.*

![cmd 88](media/images/image3129.png)

**Sales Forecasting**

10. From the sidebar, select **Sales Forecasting**.

    ![Sales Forecasting](media/images/image3132.png)

A sales forecasting model was used by Wide World Importers to validate if their approach to reduce churn and improve sales was effective.

*Here you are forecasting sales using a Regression model and deploying it to Azure ML as a service using a model registered in Azure Databricks.*

11. Review the **cmd 127** cell where model deployment is created.

    ![Model Deployement](media/images/image3136.png)

*The model is deployed to an Azure ML endpoint from which it can be consumed to predict sales.*

12. Review the **cmd 133** cell to see sales forecast using store data after launching the campaigns.

    ![Store data after the campaigns ](media/images/image3138.png)

*All these analysis using MLOps helped Wide World Importers to predict positive sales forecast and a successful year ahead*.

---

#### Task 3.2: Leverage Power BI to derive actionable insights from data in the Lakehouse. <a name="power-bi-report-to-analyse-data-in-the-Lakehouse"></a>

For this task, the date is November 1. Wide World Importers now needs to prepare for a successful Cyber Monday Sale event. Good news! The enriched datasets sourced from disparate data sources and the best performing model outputs have now been placed in the Lakehouse for Power BI consumption..

In this task, you will work with Power BI to reveal some valuable insights.

> **Note:** Please make sure you are **logged into the VM** for the following steps.

1.	Open a new web session (tab), and then navigate to: **https://powerbi.com**

2.  If prompted, enter the email account provided to you, and then select **Submit**.

![Register on Power BI](media/images/image3202.png)

*You may be redirected to a set up page.*

3. If prompted, in the **Business phone number** enter:**1234567891**, select **Next**.

>**Note:** **DO NOT** check the checkboxes at the bottom of the page.


![Enter a dummy number](media/images/imageDummyNumber.png)


4.	If prompted, select **Get Started**.

![Get Started with Power BI](media/images/image3203.png)

*You have now signed in to Power BI.*

5. In the Power BI service, in the **Navigation** pane (at the left), we have two options a) Workspaces and b) My Workspace

6. In the workspaces pane, select **My Workspace**.

![Select New Workspace](media/images/imageMyWorkspace.png)

7. To upload a Power BI Desktop file, on the **New** dropdown menu, select **Upload a file**.

> **Note:** Please make sure you are **logged into the VM** for the following steps.

![Select upload](media/images/image3208.png)

8.	Select the **Local File** tile.

![Select upload](media/images/image3209.png)

9.	In the **Open** window, navigate to the **C:\labfiles\Packaging\artifacts\reports** folder, and then select the **IgniteDemoReport.pbix** file.

10.	Select **Open**.

![Upload report upload](media/images/imageFilePath.png)

11.	Notice that a Power BI report and dataset have been added to your workspace.

12.	To open the report, select the **IgniteDemoReport**.

![Power BI report added to your workspace.](media/images/image3212.png)


13. This report has three sections:
 - Churn Analysis
 - Campaign Analytics
 - Website Analytics
 

![PowerBI Report Name](media/images/image3227.png)

14.  Lets navigate to **Churn Analysis Tab,** where we analyze the Customer Churn. This report along with Campaign Analytics and Website Analytics reports in Power BI are coming from the data Lakehouse that we created in earlier exercises.


![PowerBI Report Buttons](media/images/image3228.png)


15. In the Scatter Chart on the left, the black dots represent customers more likely to churn, and the blue dots represent customers less likely to churn.
You can notice that when the customer tenure is low and their spend amounts is also low, the customers are more likely to churn.

16. With this insight, Wide World Importers decides to target customers with lesser tenure and lesser spend amounts through their new campaigns.


![Scatter chart](media/images/image3229.png)


Now, let's go the Campaign Analytics. 


17.  Select the **Campaign Analytics** tab from the top right pane to navigate to the Campaign Analytics report.

![Select Campaigns Analytics](media/images/image3238.png)

18. In this Campaigns Analytics report, the Bar chart shows that the most popular campaigns launched are **gogreen** and **sustainablefashion**. 


![Campaigns report - Bar chart](media/images/image3239.png)


19. Click on **Website Analytics** from the top right pane to navigate to the Website Analytics Report.

![Website Analytics](media/images/imageWebsiteAnalytics.png)

Here we see an immediate problem for Wide World Importers.
The bounce rate is high. Looks like a large population of their customers/visitors leave their website without much activity.

As per this Donut chart it seems that the bounce rate is high because around 60 % of the online Customers are not happy. 

![Website Analysis Report page](media/images/image3242.png)

Let us understand more about these **not happy** customers.  

20. Click on **Not Happy** on the Donut chart to filter the report page.

![Not Happy Customers](media/images/imageDonutChart.png)
  
The **site visitors by age group** chart shows that most of the "Not Happy" Customers are in the  age group of 21 to 40. It seems that millennials make up the majority of unhappy online customers. Now, let us see what these millenials typically shop for online.
 
![Site visitors by age group](media/images/imageAgeGroup.png) 

![Repeat customers](media/images/imageSurfingBoard.png)

21.  We know that the millenials from the biggest customer segment are "Not Happy" when...

- they could not find the product they searched for on the website.
- when they were redirected to Wide World Importers website from a third party website.
- when the website user experience on their mobile phones is not good.
- they are not able find discounts on the website.

![Repeat Customers Chart](media/images/imageRepeatCustomersGraph.png)

As a result, we can say that we need to improve the user experience on the website for product search and make sure it renders correctly on mobile phones.
  
Now, let's navigate to the key influencers.
![Key influencers Chart](media/PowerBI-Images/websiteAnalytics-keyInfluencers.png)

22. Click on the **Top Segments** to show the details.

23. Click on the first bubble and see the details for Segment 1.

Segment 1 shows that it comprises of 94.5% customers who are not happy.


![Top Segments](media/images/image3248.png)


Segment 1 also shows that 94.5 % of the "not happy" customers experienced failed product searches. These are the millennials who are using their mobile devices for shopping.


![Millenials using mobile phone](media/images/imagePowerBI.png)
 

As a result of this analysis, Wide World Importers reduce their bounce rate by implementing a mobile-friendly website with fast product searches, focussing on high demand products for the millenials.
These changes not only improve the bounce rate dramatically, but they also reward Wide World Importers with uprecedented sales at their Cyber Monday Sale event. 

Now, let us see how on an ongoing basis, if there are any business needs to run adhoc time-critical queries, it can be achieved via custom queries. 

We will discuss that in more detail in the next task.

------------- 

#### Task 3.3 (OPTIONAL): Explore SQL Analytics with Azure Synapse Serverless. <a name="sql-analytics-with-synapse"></a>

Data engineers are often required to support ad hoc and time-critical queries in addition to regular, scheduled reports.

In this task, you will learn how to perform custom SQL and business intelligence workloads on the data lake. You will query the data lake by using Azure Synapse Analytics. Specifically, you will rely upon Azure Synapse serverless SQL pools, which is a service that queries data in a data lake. Itâ€™s important to understand that the data can be queried without the need to move it into tables.



1. In the Azure portal web session (tab), in the search box (located across the top of the page), enter: **Synapse Analytics**

2. In the search results pane, select **Azure Synapse Analytics**.

![Select Azure Synapse Analytics](media/images/image3302.png)

3.	In the filtered results, select the Azure Synapse resource.

![Select Azure Synapse resource](media/images/image3303.png)

*Note: You might see Synapse workspace resource name with a different suffix in your Azure Portal.*

4. In the **Open Synapse Studio** tile, select the **Open** link.


![Open Synapse studio](media/images/image3304.png)

*Synapse Studio opens in a new web session (tab).*

5. In Synapse Studio, at the left, select the **Develop** hub icon (the third from the top).

*In Synapse Studio, you can use T-SQL to directly query data in a data lake by using a serverless SQL pool. That way, you can achieve rapid data exploration.*

6. In the **Develop** pane, expand **SQL scripts**.

7. Select the **1 Query Campaign And Twitter Data Using T-SQL Language** script.

![Query Campaign And Twitter Data Using TSQL Language](media/images/image3307.png)

*You can directly query external files stored in ADLS Gen2 storage without first copying or loading the data into a specialized store. You can query the data by using familiar T-SQL syntax.*

8. In the **Connect to** dropdown list, ensure that **Built-in** is selected. 

*You will use the built-in endpoint for this service thatâ€™s provided within every Azure Synapse workspace. Itâ€™s a Synapse serverless SQL pool.*

9.	In the **Use database** dropdown list, ensure that **IgniteServerlessPool** is selected.

10.	To query the first 100 campaigns, in the script file, select lines 5-12.

11.	Select **Run**.

12.	Review the query result in the lower pane.

![Select Run](media/images/image3311.png)



We can also directly query external files stored in Azure storage without copying or loading data into a specialized store, all using familiar T-SQL dialect. This is a quick and easy way to read the content of the files without pre-configuration.
A default, built-in, endpoint for this service is provided within Synapse workspace.

*You will now create a view of the campaign file.*

13. To create a view that queries the first 100 campaigns, in the script file, select lines 35-44.

14.	Select **Run** to create a view of the Campaign file in Serverless pool.

![Select Run](media/images/image3314.png)

With the relevant metadata placed in an Azure storage account, the views will allow us to reuse those queries in other places as well such as Power BI, in conjunction with serverless SQL pool.

*Executing ad hoc queries and creating views over data in the data lake by using a Synapse serverless SQL pool is straightforward*.

We can do the same even with Databricks. Let us see how.

----  


#### Task 3.4 (OPTIONAL): Explore SQL Analytics with Azure Databricks. <a name="explore-sql-analytics-with-azure-databricks"></a>


Azure Databricks provides an environment that allows you to run quick ad hoc SQL queries on your data lake. Queries support multiple visualization types that help you to explore query results from different perspectives.

In this task, you will explore some SQL analytics features of Azure Databricks, including export and import, and running a workbook.

1. In the Azure portal web session (tab), in the search box (located across the top of the page), enter: **Azure Databricks**

2. In the search results pane, select Azure Databricks.

![Select Azure Databricks](media/images/image3402.png)

3. In the **Azure Databricks** page, select the resource that has a name starting with **databricks**.

*Note: Each student has their own unique instance of this resource. Each Azure Databricks workspace is provisioned with a full-featured development environment.*

![Select Azure Databricks resource](media/images/image3403.png)

4. In the Azure Databricks resource page, select **Launch Workspace**.

![Select Azure Databricks resource](media/images/image2104.png)

*A new web session (tab) opens.*

*You will now export a Databricks notebook in HTML format to your local machine.*

5. In the Databricks web session (tab), at the left, select the **Workspace** icon (second from the top).


6. Select **04_SQL_Analytics_On_Delta_Live_Tables** notebook in Databricks workspace.

![Go to 04_SQL_Analytics_On_Delta_Live_Tables](media/images/image3405.png)

7. Review cmd 4 cell.

**Raw Twitter Data - Bronze**

*The bronze layer stores the raw, unprocessed data from Twitter API pulls. By leaving it in its raw state, there's an option to reprocess it for different purposes in the future. Thanks to Azure Data Lake Gen2, it is possible to maintain this data for as long as possible at a very low cost.*
*The bronze layer is usually the domain of data engineers who build pipelines to refine this data and forward it into the Silver layer.*

![Raw Twitter data](media/images/image3406.png)

8. Review cmd 7 cell.

**Filtered Twitter Data - Silver**

*In the Silver layer, the raw Twitter data is curated into something more usable for data scientists.*
*They can take this cleaned up data and develop features for machine learning models as well as aggregated analytical datasets for data analysts*.

![Filtered Twitter data](media/images/image3407.png)

9. Review cmd 9 cell.

**Curated Twitter Data - Gold**

*The Gold layer is to enhance and refine the silver layer datasets even further so that it is ready for fit-for-purpose tables and views for specific analytical needs.*
*Here we've augmented the Twitter data with a machine learning model to identify the sentiment (positive, neutral, or negative) of each Tweet.*

![Curated Twitter data](media/images/image3408.png)

10. Review cmd 11 cell.

**Aggregations Are A Great User Experience Enhancement**

*By pre-emptively aggregating the data that rarely or slowly changes, it provides a great performance benefit to the end users.*
*The DLT pipeline performs this aggregation of hashtag counts by the geolocation of the Tweets.*
By updating this everytime more Tweets are ingested, it is possible to keep the aggregation table up-to-date and then quickly consume and visualize it in tools like Power BI.One nice feature of Databricks notebooks is if a cell produces a DataFrame output (like the one below), you can also profile the data as well as generate quick visualizations. Throw in Markdown and comments and notebooks are a super convenient way to collaborate and communicate with your team, leadership, customers, and other stakeholders.

![Aggregations Are A Great User Experience Enhancement](media/images/image3410.png)


11. Review cmd 20 cell.

**Z-Order Optimization**

*Z-Ordering is a technique to co-locate related information in the same set of files.*
*This co-locality is automatically used by Delta Lake on Databricks data-skipping algorithms.*
*This behavior dramatically reduces the amount of data that Delta Lake on Databricks needs to read.*

![Z-Order Optimization](media/images/image3411.png)

12. Review cmd 22 cell.

**Caching**

*Caching reduces scanning of the original files in future queries. It caches contents of a table in Apache Spark cache. If a query is cached, then a temperoray view is created for this query.*

![Caching](media/images/image3412.png)

13.  Review cmd 24 cell.

**Time Travel**

*In the audit history it is possible to see the history of the different versions of the table as well as load and display any of those versions.*

![Time Travel](media/images/image3413.png)
