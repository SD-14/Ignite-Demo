# Build an open standard data Lakehouse by using Azure Synapse Analytics and Azure Databricks

**The estimated time to complete this lab is 45 minutes.**

## Table of contents

Exercise 1: Data ingestion from a spectrum of analytical and operational data sources into the Lakehouse.
	- Task 1.1: Explore a Streaming data and analytics pipeline using ADX for a near real time analytics scenario.
	- Task 1.2: Explore a few Synapse pipelines that ingest raw data from analytical data sources to the Bronze layer of the Data Lake.
	- Task 1.3: Explore a few Synapse pipelines that ingest raw data from operational data sources to the Bronze layer of the Data Lake.
	
Exercise 2: Explore offline data and analytics pipeline using open Delta format and Azure Databricks Delta Live Tables. Stitch streaming and non-streaming data landed earlier to create a combined data product to build a simple Lakehouse.
  - Task 2.1: Set up Azure Databricks environment.
	- Task 2.2: Review sentiment analysis model training.
	- Task 2.3: Create a Delta Live Table pipeline.
Exercise 3: Explore Machine Learning and Business Intelligence scenarios on the Lakehouse.

	- Task 3.1: Review MLOps pipeline using the Azure Databricks managed MLflow.
	- Task 3.2: Leverage Power BI to derive actionable insights from data in the Lakehouse.
	- Task 3.3 (OPTIONAL): Explore SQL Analytics with Azure Synapse Serverless.
	- Task 3.4 (OPTIONAL): Explore SQL Analytics with Azure Databricks.
	
Exercise 4: Glimpse of Purview to govern the overall data and analytics estate.

----

This lab explores an end-to-end implementation of an open standard data Lakehouse. You have the opportunity to explore raw data ingestion from disparate data sources that transforms data by using Delta Live Tables in Azure Databricks. The ingestion creates data products that can be further leveraged by data science, machine learning, and business intelligence applications.
This lab will demonstrate the capability of Lakehouse to serve as a single platform for managing and supporting data and analytics needs.
You will work through an example of a real world implementation for the fictitious Wide World Importers Enterprise.

Wide World Importers is a brick-and-mortar retailer that has hundreds of stores worldwide and a fast-growing online store. It sells a wide variety of consumer merchandise, including sunglasses, sports shoes, watches, wallets, books, and various beach products.

The lab scenario starts on May 30th, 2021. The company's new CEO, April, recently noticed negative trends in their KPIs, including:

- High customer churn
- Declining sales revenue
- High bounce rate on their website
- Poor customer experience

In a purely reactive mode, as soon as the company saw these adverse KPI trends, they launched some traditional campaigns. On September 5 (Labor Day), the results of those campaigns were received. The company noticed that the campaigns failed to be effective.

So, April talked with Rupesh, the chief data officer (CDO), about these adverse KPI trends and recommended a data-driven approach.

In this Exercise you will act as a Data Engineer. Rupesh would like you to improve the above KPIs using the following requirements:

- Leverage data from the past, present, and future (Volume).
- Enable quick turnaround time (Velocity).
- Support open standards data format (Variety).

Here is the visual representation of the exercise that we are going to perform in this lab.

![Lab exercises](https://github.com/SD-14/Ignite-Demo/blob/main/media/imageLabExercises.png?raw=true)

You will experience creation of a simple integrated, open and governed Data Lakehouse foundation using the Microsoft Analytics Solution pattern. 

In this lab we will cover the following: 

1. As a first step we will look at Data ingestion from a spectrum of analytical and operational data sources into the Lakehouse. We start with Streaming data and analytics pipeline using ADX for a near real time analytics scenario followed by Synapse pipelines that ingest raw data from analytical/ operational data sources to the Bronze layer. 

2. The second step is to explore offline data and analytics pipeline using open Delta format and Azure Databricks Delta Live Tables. We will stitch streaming and non-streaming data (landed earlier), to create a combined data product to build a simple Lakehouse.

3. In the third step, we explore ML and BI scenarios on the Lakehouse. Here we will review MLOps pipeline using the Azure Databricks managed MLflow with Azure ML. Then using Power BI with Synapse serverless capabilities we will derive actionable insights. Last, but not least, we will leverage Purview for data governance.

[Click Through Demo](https://content.cloudguides.com/guides/Build%20an%20open%20standard%20data%20lakehouse)
