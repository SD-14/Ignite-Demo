### Exercise 1: Data ingestion from a spectrum of analytical and operational data sources into the Lakehouse. 

As the data engineer at Wide World Importers, you will start by landing data from a variety of sources into the Lakehouse. This data will be further cleansed, processed, and conformed by using Azure Databricks and Delta Live Tables. You will prepare the data for downstream consumption by data scientists and business intelligence analysts. Data sources include data related to its customers, products, marketing campaigns, social media, and sales transactions. This data is often generated in raw files format such as CSV, JSON, unstructured files, and even images. A lot of existing data is historical data.

To boost customer satisfaction, gain a competitive advantage, and ultimately drive revenue growth, Wide World Importers wants to analyze the data to obtain meaningful insights related to their customers, marketing campaigns, and sales forecasts. However, their immediate challenge is to generate and use near real-time streaming data. So, they installed IoT devices in their stores to analyze customer shopping patterns and thermostat readings. They also set up Azure Data Explorer (ADX) with anomaly detection to correlate in-store traffic and store temperatures. As a result, they now have a large volume of real-time streaming data related to in-store traffic, temperature readings and anomaly detection.

In this exercise, you will explore how to ingest near real-time data into the Lakehouse and derive meaningful insights.


#### Task 1.1: Explore a Streaming data and analytics pipeline using ADX for a near real time analytics scenario. 

Wide World Importers wants its customers to have a pleasant in-store shopping experience. Maintaining the optimal temperature in stores and wine coolers is one way to accomplish this objective.

Consider that the Black Friday Sale in-store event has just started at 6:00 AM EST, and customers are arriving in large numbers at the Miami store. As described earlier, thermostat data from the stores is streamed in real-time to an Azure Event Hub and then into an Azure Data Explorer (ADX) pool for analysis.

In this task, you will use ADX to explore thermostat data from the stores streamed in near real time to an Azure Event Hub. 

1. In the search results pane, select **Resource groups**.

![In the search results pane, select the Resource group](media/images/image1107.png)

2. Select the resource group that has a name starting with **analyticsSolution-**.

  >**Note:** Each student has their own unique instance of this resource.

![In the filtered results, select the resource group](media/images/image1109.png)

3. In the resources filter box for resources, search for **app**.

4. In the filtered results, select the App Service.

![Select the app service](media/images/imageAppServices.png)

12. Select **Browse** (on the top left )

*This action will start the data simulation required to execute this task successfully*.

>**Note:** You will be redirected to a new web session (tab) with the **error 404 (Page Not Found)** .

![Select Browse](media/images/imageclickBrowse.png)

13. Return to the Azure Portal session.

14.	In the resources filter box for resources, search for **Synapse**.

15.	In the filtered results, select the Azure Synapse resource.

![In the filtered results, select the Azure Synapse resource](media/images/image1114.png)

*Note: You might see Synapse workspace resource name with a different suffix in your Azure Portal.*

16. In the Open Synapse Studio tile, select the Open link.


![Open Synapse studio](media/images/image1115.png)

>**Note:** Synapse Studio opens in a new web session (tab).

17.	In Synapse Studio, at the left, select the **Data** hub icon (the second from the top).

18.	In the **Data** pane, expand **Data Explorer Databases (Preview).**

19.	Expand the **ignitekustopool** Data Explorer pool.

20. Select the **ellipses** (the three dots next to the data explorer pool).
>**Note:** If you do not see the ellipses, expand the Data pane by dragging it to the right. 

![Expand Data Explorer Pool](media/images/image1118.png)

21. Select **Open in Azure Data Explorer**.

>**Note:** Please remember to execute step 20.

*This will open Azure Data Explorer in a new web session (tab).*

![Open Azure Data Explorer](media/images/imageOpenADX.png)

*For this lab, an ADX pool has already been created in the Azure Synapse workspace.*

*By using ADXâ€™s powerful Kusto Query Language (KQL), you can ensure that the thresholds you have set for each device in the store is being met.*


>**Note:** Other Azure services use KQL for analytical queries. These services include Azure Monitor logs, Application Insights, and Microsoft Defender for Endpoint.


22.	In Azure Data Explorer Studio, in the left pane, select the **Data** hub icon.

23.	In the **Data Management** page, select the **Ingest data** action.

![Paste the URI into the address bar](media/images/image1127.png)

24.	In the **Destination** tab, in **Cluster** dropdown list, select the Data Explorer pool.
     
25. In the Database dropdown, select **IngiteDB**. Then in the **Table** textbox, enter/type **Thermostat**.

![Click Next](media/images/image1131.png)

26.	Select **Next: Source.**

>**Note:** In Step 24 if you do NOT see any Azure Data Explorer Pool in **Cluster** dropdown, select **Add Cluster** then follow steps a-f below.

![Add Cluster](media/images/imageAddCluster.png)

a. Navigate to the Azure Synapse web session (tab), select **Manage** hub from the left pane.

b. Select the **Data Explorer pools (preview)** from the **Analytics pool** pane.

c. Select the **igniteKustopool-**.

![manage tab](media/images/imageManageHub.png)

d. Under the **Query endpoint** copy the **URL** and select **Close** to close the pane.

![Copy URI](media/images/imageCopyURI.png)

e. Go back to the **Azure Data Explorer** tab, in the **Connection URI**, enter the URI copied above.

f. Select **Add**, continue with Step 25.

![click Add](media/images/imageClickAdd.png)

27.	In the **Source** tab, in the **Source** dropdown list, select **Event Hub**.

28.	In **Subscription** dropdown list, select your subscription.

29.	In the **Event Hub namespace** dropdown list, select Event Hub that has a name starting with **adx-thermostat-occupancy-**.

30.	In the **Event Hub** dropdown list, select **thermostat**.

31.	In **Data connection name** dropdown list, select **IgniteDB-thermostat**.

32.	In the **Consumer group** dropdown list, select **$Default**.

33.	In **Compression** dropdown list, select **None**.

34.	Select **Next: Schema**.

![Click Next](media/images/image1139.png)

35.	In the **Schema** tab, wait until the data preview loads (about 20 seconds).

36.	Review the event data, which comprises thermostat measures from different devices.

37.	In the **Data** format dropdown list, select **JSON.**

>**Note:** Please remember to execute step 37. 

38.	Select **Next: Start ingestion.**

![Click Next](media/images/image1143.png)

39.	Confirm that the continuous ingestion from Event Hub has been established, and then select **Close** (located at the bottom of the page).

![Select Close](media/images/image1144.png)

40. Return to the Synapse Studio web session (tab).

41. In Synapse Studio, at the left, select the **Develop** hub icon (the third from the top).

42. In the **Develop** pane, expand **KQL scripts**.

43. Select the **ThermostatOccupancyScript** script.

![Select the ThermostatOccupancyScript Sript](media/images/image1148.png)

44. In the **Connect to** dropdown list, select the data explorer pool.

>**Note:** If required, collapse the panes on the left using the << icon at the top right of each pane.

45. In the **Use database** dropdown list, select **IgniteDB**.

46.	Select the query (lines 4-8) that is commented as **What is the average temp every minute?**.

*The query retrieves the average temperature per minute for a thermostat device (TH005) for the Miami store.*

47.	Select **Run**. 

48.	In the **Results** pane (located along the bottom), review the query result expressed as a chart.

>**Note:** In case your query does not run successfully, chances are that the thermostat table was not created successfully in previous steps. You may have to create that table with a different name e.g. Thermostat1, update the KQL query accordingly and re-execute the KQL query.  

![Review the query result ](media/images/image1152.png)

*Your graph may appear slightly different than the one shown above. It may take up to 60 seconds to load.*

49.	Notice that the temperature in the Miami store is oscillating between 65 and 70 degrees Fahrenheit. Also notice, that as soon as any unanticipated spike occurs in the store temperature, it is automatically cooled down using ADX.

----

  

#### Task 1.2: Explore a few Synapse pipelines that ingest raw data from analytical data sources to the Bronze layer of the Data Lake. <a name="analytical-sources"></a>

  
Your next challenge is to ingest historical data from a spectrum of data sources.  

In this task you will ingest campaigns data from Snowflake and customer churn data from Teradata into the data lake.

1. Return to the Synapse Studio web session (tab).

2. In Synapse Studio, at the left, select the **Integrate** hub icon (the fourth from the top).

3. In the **Integrate** pane, expand **Pipelines**.

4. Expand the **1 Enterprise Data Sources In The Lake** folder.

5. Expand the **Landing Analytical Store Data** folder.

6. Select the **Campaigns Data From Snowflake** pipeline.

>**Note:** If required, collapse the panes on the left using the << icon at the top right of each pane.

![Campaigns data from Snowflake ](media/images/image1206.png)


*The ***Campaigns Data from Snowflake*** pipeline has two activities. The first one runs a lookup of data at the source Snowflake connection. The next activity brings that data into the Bronze layer in ADLS Gen2.*


7.	In the pipeline designer, select Lookup activity.

8.	In the pane below, select the **Settings** tab.

9.	In the **Source dataset** dropdown list, notice that **SnowflakeTable** is selected.

![Source Dataset](media/images/image1209.png)

10.	In the pipeline designer, select **Copy data** activity.

11.	In the pane below, select the **Sink** tab.

12.	In the **Sink dataset** dropdown list, notice that **SnowflakeCampaignsData** is selected.

![Sink Dataset](media/images/image1212.png)

*Similarly, the next pipeline is designed to ingest customer churn data from Teradata and Twitter data to the data lake.*

>**Note:** The image is for informational purposes only. Due to time constraints, we will not explore it in the lab.

![CustomerChurn Data From Teradata](media/images/image1213.png)

----

  

#### Task 1.3: Explore a few Synapse pipelines that ingest raw data from operational data sources to the Bronze layer of the Data Lake. <a name="operational-sources"></a>

  
In this task, you will explore the design of a Synapse pipeline that is designed to ingest raw data coming from various operational sources into the data lake.

1.	In the **Integrate** pane, in the **Landing Operational Store Data** folder, select the **Store Transactions Data from SQL DB** pipeline.

![Landing Operational Store Data](media/images/image1309.png)

*The **Store Transactions Data from SQL DB** pipeline has two activities. The first one runs a lookup of data at the source Azure SQL Database connection. The next activity brings that data into the Bronze layer in ADLS Gen2*.

>**Note:** If required, collapse the panes on the left using the << icon at the top right of each pane.

2.	In the pipeline designer, select the Copy data activity.

3.	In the pane below, select the **Sink** tab.

4.	In the **Sink dataset** dropdown list, notice that **DestinationDataset** is selected.

![Sink dataset](media/images/image1312.png)

*Similarly, the next pipeline is designed to ingest Sales data from Oracle to the data lake.*

>**Note:** The image is for informational purposes only. Due to time constraints, we will not explore it in the lab.

![Sales Data](media/images/image1302.png)


Congratulations! As a data engineer you have successfully ingested streaming near real time as well as historical data into data lake for Wide World importers.
